<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Vector of Random Variables &#8212; Regression Analysis Notes 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=61cd365c" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <script src="_static/documentation_options.js?v=d45e8c67"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multivariate Normal Distribution" href="2_multivariate_normal_distribution.html" />
    <link rel="prev" title="Matrix Preparation" href="0_matrix_preparation.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="vector-of-random-variables">
<h1>Vector of Random Variables<a class="headerlink" href="#vector-of-random-variables" title="Link to this heading">¶</a></h1>
<p>In this chapter, we will explore the expectation and covariance for a vector of random variables. Then we derive the function of a random vector as quadratic form and discuss its mean and variance. We also cover the independence of two random vectors.</p>
<section id="expectation">
<h2>Expectation<a class="headerlink" href="#expectation" title="Link to this heading">¶</a></h2>
<p><strong>Definition 1.1</strong>: Let <span class="math notranslate nohighlight">\(Z=(Z_{ij})\)</span>. The expectation of <span class="math notranslate nohighlight">\(Z\)</span> is <span class="math notranslate nohighlight">\(\mathbb{E}(Z)=(\mathbb{E}(Z_{ij}))\)</span>.</p>
<p><strong>Theorem 1.1</strong>: Let <span class="math notranslate nohighlight">\(Z\)</span> be an <span class="math notranslate nohighlight">\(m\times n\)</span> random matrix. Let <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> be <span class="math notranslate nohighlight">\(l\times m\)</span>, <span class="math notranslate nohighlight">\(n\times p\)</span>, and <span class="math notranslate nohighlight">\(l\times p\)</span> matrices of constant, respectively. Then</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(AZB + C) = A\mathbb{E}(Z)B + c.
\]</div>
<p><strong>Proof</strong>: We prove this based on the <a class="reference external" href="https://brilliant.org/wiki/linearity-of-expectation/#:~:text=Linearity%20of%20expectation%20is%20the,of%20whether%20they%20are%20independent.">linearity of expectation</a> of a random variable (e.g. <span class="math notranslate nohighlight">\(\mathbb{E}(ax + b) = a\mathbb{E}(x) + b\)</span>)).</p>
<p><strong>Corollary</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(m\times 1\)</span> random vector. Then <span class="math notranslate nohighlight">\(\mathbb{E}(AX + C) = A\mathbb{E}(X) + C\)</span>.</p>
</section>
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Link to this heading">¶</a></h2>
<p><strong>Definition 1.2</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be <span class="math notranslate nohighlight">\(m\times 1\)</span> and <span class="math notranslate nohighlight">\(n\times 1\)</span> random vectors, respectively. Then</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(X, Y) = (\text{cov}(X_i, Y_j))
\]</div>
<p><strong>Definition 1.3</strong>: The <em>variance (dispersion) matrix</em> of <span class="math notranslate nohighlight">\(X\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{cov}(X, X) = \text{var}(X) = \begin{bmatrix}
\text{var}(X_1) &amp; \text{cov}(X_1, X_2) &amp; \cdots &amp; \text{cov}(X_1, X_n) \\
\text{cov}(X_2, X_1) &amp; \text{var}(X_2) &amp; \cdots &amp; \text{cov}(X_2, X_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\text{cov}(X_n, X_1) &amp; \text{cov}(X_n, X_2) &amp; \cdots &amp; \text{var}(X_n)
\end{bmatrix}.
\end{split}\]</div>
<p><strong>Theorem 1.2</strong>: Let <span class="math notranslate nohighlight">\(\mathbb{E}(X) = a\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}(Y) = b\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(X, Y) = \mathbb{E}[(X - a)(Y - b)'] = \mathbb{E}(XY') - ab'.
\]</div>
<p><strong>Proof</strong>: We prove this based on <em>Theorem 1.1</em> of <a class="reference internal" href="#expectation"><span class="std std-ref">expectation</span></a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Covariance#Definition">definition of covariance</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{cov}(X, Y) &amp; = \mathbb{E}[(X - \mathbb{E}(X))(Y - \mathbb{E}(Y))'] \\
&amp; = \mathbb{E}[(X - a)(Y - b)'] \\
&amp; = \mathbb{E}[XY' - aY' - Xb' + ab'] \\
&amp; = \mathbb{E}[XY'] - a\mathbb{E}[Y'] - \mathbb{E}[X]b' + ab' \\
&amp; = \mathbb{E}[XY'] - ab' - ab' + ab' \\
&amp; = \mathbb{E}[XY'] - ab'
\end{aligned}
\end{split}\]</div>
<p><strong>Corollary</strong>: <span class="math notranslate nohighlight">\(\text{var}(X) = \mathbb{E}(XX') - aa'\)</span>.</p>
<p><strong>Proof</strong>: <span class="math notranslate nohighlight">\(\text{var}(X) = \text{cov}(X, X) = \mathbb{E}(XX') - aa'\)</span>.</p>
<p><strong>Theorem 1.3</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be <span class="math notranslate nohighlight">\(n\times 1\)</span> random vectors. Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be <span class="math notranslate nohighlight">\(m\times n\)</span> matrices of constants. Then</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(AX, BY) = A\text{cov}(X, Y)B'
\]</div>
<p><strong>Proof</strong>: We prove this in a similar way as <em>Theorem 1.2</em>. Suppose <span class="math notranslate nohighlight">\(\mathbb{E}(X) = a\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}(Y) = b\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{cov}(AX, BY) &amp; = \mathbb{E}[(AX - \mathbb{E}(AX))(BY - \mathbb{E}(BY))'] \\
&amp; = \mathbb{E}[(AX - Aa)(BY - Bb)'] \\
&amp; = \mathbb{E}[AX(BY)'] - Aa(Bb)' \\
&amp; = \mathbb{E}[AXY'B'] - Aab'B' \\
&amp; = A\mathbb{E}[XY']B' - Aab'B' \\
&amp; = A(\mathbb{E}[XY'] - ab')B' \\
&amp; = A\text{cov}(X, Y)B'
\end{aligned}
\end{split}\]</div>
<p><strong>Corollary</strong>: <span class="math notranslate nohighlight">\(\text{var}(AX) = A\text{var}(X)A'\)</span>.</p>
<p><strong>Proof</strong>: <span class="math notranslate nohighlight">\(\text{var}(AX) = \text{cov}(AX, AX) = A\text{cov}(X, X)A' = A\text{var}(X)A'\)</span>.</p>
<p><strong>Theorem 1.4</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(n\times 1\)</span> random vector. Then <span class="math notranslate nohighlight">\(\text{var}(X)\)</span> is non-negative definite. Furthermore, if no element of <span class="math notranslate nohighlight">\(X\)</span> is a linear combination of the remaining elements (that is, there does not exist <span class="math notranslate nohighlight">\(a\in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(a\neq 0\)</span> and <span class="math notranslate nohighlight">\(b\in\mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(a'X = b\)</span> for all values of <span class="math notranslate nohighlight">\(X = x\)</span>), then <span class="math notranslate nohighlight">\(\text{var}(X)\)</span> is positive definite.</p>
<p><strong>Proof</strong>: Based previous corollary, given any <span class="math notranslate nohighlight">\(c\in \mathbb{R}^n\)</span>, since the variance is non-negative, we have</p>
<div class="math notranslate nohighlight">
\[
0 \leq \text{var}(c'X) = c'\text{var}(X)c
\]</div>
<p>Then <span class="math notranslate nohighlight">\(\text{var}(X)\)</span> is non-negative definite.</p>
<p>Based on the definition of variance, the equality holds if any only if <span class="math notranslate nohighlight">\(c = 0\)</span> or <span class="math notranslate nohighlight">\(c'X = b\)</span> when <span class="math notranslate nohighlight">\(c \neq 0\)</span> for some <span class="math notranslate nohighlight">\(b\in \mathbb{R}\)</span>. Since our condition rules out the second possibility, <span class="math notranslate nohighlight">\(0 = \text{var}(c'X)\)</span> if and only if <span class="math notranslate nohighlight">\(c = 0\)</span>. Therefore, <span class="math notranslate nohighlight">\(\text{var}(X)\)</span> is positive definite.</p>
</section>
<section id="quadratic-form">
<h2>Quadratic Form<a class="headerlink" href="#quadratic-form" title="Link to this heading">¶</a></h2>
<p><strong>Definition</strong>: A function <span class="math notranslate nohighlight">\(f(x_1, x_2, \dots, x_n)\)</span> is a <em>quadratic form</em> with matrix <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times n}\)</span> in variables <span class="math notranslate nohighlight">\(X = (X_1, X_2, \dots, X_n)'\)</span> if</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f(X) &amp; = X'AX \\
&amp; = \sum_{i = 1}^n\sum_{j = 1}^n a_{ij}X_iX_j
\end{aligned}
\end{split}\]</div>
<section id="mean-of-quadratic-forms">
<h3>Mean of Quadratic Forms<a class="headerlink" href="#mean-of-quadratic-forms" title="Link to this heading">¶</a></h3>
<p><strong>Theorem 1.5</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(n\times 1\)</span> random vector and <span class="math notranslate nohighlight">\(A\)</span> be a symmetric matrix of constants. Let <span class="math notranslate nohighlight">\(\mathbb{E}(X) = \mu\)</span> and <span class="math notranslate nohighlight">\(\text{var}(X) = \Sigma\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(X'AX) = \text{tr}(A\Sigma) + \mu'A\mu.
\]</div>
<p><strong>Proof</strong>: By definition, <span class="math notranslate nohighlight">\(\Sigma\)</span> is symmetric. Then we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[X'AX] &amp; = \mathbb{E}\Bigg[\sum_{i = 1}^n\sum_{j = 1}^n a_{ij} X_i X_j\Bigg] \\
&amp; = \sum_{i = 1}^n\sum_{j = 1}^n a_{ij}\mathbb{E}[X_iX_j] \\
&amp; = \sum_{i = 1}^n\sum_{j = 1}^n a_{ij}\big(\text{cov}(X_i, X_j) + \mathbb{E}[X_i]\mathbb{E}[X_j]\big) \\
&amp; = \sum_{i = 1}^n\sum_{j = 1}^n a_{ij}(\Sigma_{ij} + \mu_i\mu_j) \\
&amp; = \sum_{i = 1}^n\sum_{j = 1}^n a_{ij}\Sigma_{ji} + \sum_{i = 1}^n\sum_{j = 1}^na_{ij}\mu_i\mu_j \\
&amp; = \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
\end{split}\]</div>
<p><strong>Corollary</strong>: Let <span class="math notranslate nohighlight">\(b\)</span> be an <span class="math notranslate nohighlight">\(n\times 1\)</span> vector of constants. Then</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}((X - b)'A(X - b)) = \text{tr}(A\Sigma) + (\mu - b)'A(\mu - b).
\]</div>
<p><strong>Proof</strong>: Since <span class="math notranslate nohighlight">\(b\)</span> is constant, we have <span class="math notranslate nohighlight">\(\mathbb{E}(X - b) = \mathbb{E}(X) - b = \mu - b\)</span> and <span class="math notranslate nohighlight">\(\text{var}(X - b) = \text{var}(X) = \Sigma\)</span>. Then apply <em>Theorem 1.5</em>, we could get the result.</p>
<p><strong>Example</strong>: Let <span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_n\overset{\text{i.i.d.}}{\sim}(\mu, \sigma^2)\)</span>. Find <span class="math notranslate nohighlight">\(\mathbb{E}(Q)\)</span> for</p>
<div class="math notranslate nohighlight">
\[
Q = (X_1 - X_2)^2 + (X_2 - X_3)^2 + \cdots + (X_{n - 1} - X_n)^2
\]</div>
<p><strong>Solution</strong>: Since <span class="math notranslate nohighlight">\(X_1 - X_2 = (X_1 - \mu) - (X_2 - \mu)\)</span>, <span class="math notranslate nohighlight">\(Q\)</span> does not depend on <span class="math notranslate nohighlight">\(\mu\)</span>. We assume <span class="math notranslate nohighlight">\(\mu = 0\)</span>. Then we expand <span class="math notranslate nohighlight">\(Q\)</span> to get the quadratic form</p>
<div class="math notranslate nohighlight">
\[
Q = X_1^2 + 2\sum_{i = 2}^{n - 1}X_i^2 + X_n^2 - 2\sum_{i = 1}^{n - 1}X_iX_{i + 1} = X'AX
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix}
1 &amp; -1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
-1 &amp; 2 &amp; -1 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 2 &amp; -1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; -1 &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>Then we apply <em>Theorem 1.5</em> to get <span class="math notranslate nohighlight">\(\mathbb{E}(Q) = \text{tr}(A\Sigma) = (2n - 2)\sigma^2\)</span>.</p>
<p><strong>Example</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(n\times 1\)</span> random vector with <span class="math notranslate nohighlight">\(\mathbb{E}(X_i)=\mu\)</span> and <span class="math notranslate nohighlight">\(\text{var}(X) = \Sigma\)</span>, where <span class="math notranslate nohighlight">\(\sigma_{ii} = \sigma^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_{ij} = \rho\sigma^2\)</span> with (<span class="math notranslate nohighlight">\(0\leq \rho\leq 1\)</span>) for <span class="math notranslate nohighlight">\(i\neq j\)</span>. Show that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(Q) = \sigma^2(1 - \rho)(n - 1)\text{ for }Q = \sum_{i = 1}^n(X_i -\bar{X})^2.
\]</div>
<p><strong>Solution</strong>: Since <span class="math notranslate nohighlight">\(X_i - \bar{X} = (X_i - \mu) - \frac{1}{n}\sum_{i = 1}^n(X_i - \mu)\)</span>, <span class="math notranslate nohighlight">\(Q\)</span> does not depend on <span class="math notranslate nohighlight">\(\mu\)</span>. We assume <span class="math notranslate nohighlight">\(\mu = 0\)</span>. Then we expand <span class="math notranslate nohighlight">\(Q\)</span> to get the quadratic form</p>
<div class="math notranslate nohighlight">
\[
Q = \sum_{i = 1}^n X_i^2 - 2\bar{X}\sum_{i = 1}^n X_i + n\bar{X}^2 = \sum_{i = 1}^n(1 - \frac{1}{n})X_i^2 + \sum_{i &lt; j}-\frac{2}{n}X_iX_j = X'AX,
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix}
1 - \frac{1}{n} &amp; -\frac{1}{n} &amp; \cdots &amp; -\frac{1}{n} \\
-\frac{1}{n} &amp; 1 - \frac{1}{n} &amp; \cdots &amp; -\frac{1}{n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-\frac{1}{n} &amp; -\frac{1}{n} &amp; \cdots &amp; 1 - \frac{1}{n}
\end{bmatrix}
\end{split}\]</div>
<p>Also we have the variance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \sigma^2 \begin{bmatrix}
1 &amp; \rho &amp; \cdots &amp; \rho \\
\rho &amp; 1 &amp; \cdots &amp; \rho \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho &amp; \rho &amp; \cdots &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>Then we apply <em>Theorem 1.5</em> to get</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(Q) = \text{tr}(A\Sigma) = \sigma^2(1 - \rho)\text{tr}(A) = \sigma^2(1 - \rho)(n - 1).
\]</div>
</section>
<section id="variance-of-quadratic-forms">
<h3>Variance of Quadratic Forms<a class="headerlink" href="#variance-of-quadratic-forms" title="Link to this heading">¶</a></h3>
<p><strong>Theorem 1.6</strong>: Let <span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_n\)</span> be independent with means <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \dots, \theta_n\)</span>, common variance <span class="math notranslate nohighlight">\(\mu_2\)</span> and common third and fourth central moments <span class="math notranslate nohighlight">\(\mu_3\)</span> and <span class="math notranslate nohighlight">\(\mu_4\)</span> (that is, <span class="math notranslate nohighlight">\(\mu_k = \mathbb{E}((X_i - \theta_i)^k)\)</span> for <span class="math notranslate nohighlight">\(k\in\mathbb{N}\)</span>), respectively. Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> symmetric matrix of constants and <span class="math notranslate nohighlight">\(a\)</span> be a column vector of the diagonal of <span class="math notranslate nohighlight">\(A\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\text{var}(X'AX) = (\mu_4 - 3\mu_2^2)a'a + 2\mu_2^2\text{tr}(A^2) + 4\mu_2\theta'A^2\theta + 4\mu_3\theta'Aa.
\]</div>
<p><strong>Proof</strong>:</p>
<p><strong>Example</strong>: Let <span class="math notranslate nohighlight">\(Z_i\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0, \mu_2)\)</span>. Then <span class="math notranslate nohighlight">\(\mu_3 = 0, \mu_4 = 3\mu_2^2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\text{var}(Z'AZ) = 2\mu_2^2\text{tr}(A^2).
\]</div>
</section>
</section>
<section id="independence">
<h2>Independence<a class="headerlink" href="#independence" title="Link to this heading">¶</a></h2>
<p><strong>Definition</strong>: Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random vectors. Then <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if</p>
<div class="math notranslate nohighlight">
\[
f_{X, Y}(x, y) = f_X(x)f_Y(y) \text{ or } F_{X, Y}(x, y) = F_X(x)F_Y(y) \text { for all }x, y.
\]</div>
<p><strong>Proposition</strong>: If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, <span class="math notranslate nohighlight">\(\text{cov}(X, Y) = 0\)</span>.</p>
<p><strong>Proof</strong>: Given that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}(XY) &amp; = \iint_{x,y}xyf_{X,Y}(x, y)dxdy \\
&amp; = \iint_{x, y}xyf_X(x)f_Y(y)dxdy \\
&amp; = \int_xxf_X(x)dx \cdot \int_yyf_Y(y)dy \\
&amp; = \mathbb{E}(X)\mathbb{E}(Y)
\end{aligned}
\end{split}\]</div>
<p>Then by <em>Theorem 1.2</em> in <a class="reference internal" href="#expectation"><span class="std std-ref">expectation</span></a>, we have <span class="math notranslate nohighlight">\(\text{cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = 0\)</span>.</p>
<p><strong>Example</strong>: Let <span class="math notranslate nohighlight">\(X\sim \mathcal{U}(-1, 1)\)</span>, and let <span class="math notranslate nohighlight">\(Y = X^2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\text{cov}(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = \mathbb{E}(X^3) - \mathbb{E}(X)\mathbb{E}(X^2) = 0
\]</div>
<p>However, it is clear that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are not independent. Therefore, we could derive a proposition</p>
<p><strong>Proposition</strong>: If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, <span class="math notranslate nohighlight">\(\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))\)</span>. However, the opposite direction does not always hold.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Regression Analysis Notes</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="0_matrix_preparation.html">Matrix Preparation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vector of Random Variables</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#expectation">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#covariance">Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quadratic-form">Quadratic Form</a></li>
<li class="toctree-l2"><a class="reference internal" href="#independence">Independence</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2_multivariate_normal_distribution.html">Multivariate Normal Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_linear_regression.html">Linear Regression</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="0_matrix_preparation.html" title="previous chapter">Matrix Preparation</a></li>
      <li>Next: <a href="2_multivariate_normal_distribution.html" title="next chapter">Multivariate Normal Distribution</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Xinpei Shen.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.3.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="_sources/1_vectors_of_random_variables.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>